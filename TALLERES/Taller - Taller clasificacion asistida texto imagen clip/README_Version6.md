# ğŸ¤– ClasificaciÃ³n Visual y Verbal de ImÃ¡genes con CLIP (OpenAI) en Google Colab

## ğŸ“… Fecha
`2025-07-04`

---

## ğŸ¯ Objetivo del Notebook

Explorar el modelo CLIP de OpenAI para clasificar imÃ¡genes, combinando procesamiento visual y comprensiÃ³n textual. Se realiza una clasificaciÃ³n de imÃ¡genes (por ejemplo, de un auto) usando etiquetas escritas en lenguaje natural, mostrando probabilidades para cada etiqueta candidata.

---

## ğŸ§  Conceptos Aprendidos

- [x] Uso del modelo CLIP para clasificaciÃ³n de imÃ¡genes con descripciones textuales libres
- [x] Proceso de instalaciÃ³n y carga de modelos pre-entrenados en Colab
- [x] Preprocesamiento de imÃ¡genes y textos para modelos de visiÃ³n+lenguaje
- [x] InterpretaciÃ³n de salidas: probabilidades y predicciones
- [x] VisualizaciÃ³n de resultados con Matplotlib

---

## ğŸ”§ Herramientas y Entornos

- Google Colab (Python 3, GPU opcional)
- [CLIP (OpenAI)](https://github.com/openai/CLIP)
- torch, torchvision, numpy, PIL, matplotlib

---

## ğŸ“ Estructura del Notebook

```
Visual_y_Verbal_ClasificaciÃ³n_de_ImÃ¡genes_con_CLIP.ipynb
```

---

## ğŸ§ª ImplementaciÃ³n

### ğŸ”¹ Etapas realizadas

1. InstalaciÃ³n de dependencias: torch, torchvision, ftfy, regex, tqdm, y CLIP desde GitHub.
2. Carga del modelo CLIP y selecciÃ³n de dispositivo (CPU o GPU).
3. DefiniciÃ³n de etiquetas en lenguaje natural (ej: `"a blurry cat"`, `"a fast sports car"`, `"dog"`, `"horse"`, `"a car"`, `"a tree"`).
4. Preprocesamiento de la imagen de entrada y de las etiquetas.
5. ExtracciÃ³n de caracterÃ­sticas y cÃ¡lculo de probabilidades de correspondencia entre la imagen y cada etiqueta.
6. VisualizaciÃ³n del resultado y de las probabilidades con Matplotlib.

---

### ğŸ”¹ Fragmento real del cÃ³digo para clasificaciÃ³n

```python
import clip
import torch
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)
labels = ["a blurry cat", "a fast sports car", "dog", "horse", "a car", "a tree"]
image = preprocess(Image.open("/content/car.jpeg")).unsqueeze(0).to(device)
text = clip.tokenize(labels).to(device)
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    logits_per_image, _ = model(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()
```

**PredicciÃ³n y visualizaciÃ³n:**

```python
predicted_label_index = np.argmax(probs)
predicted_label = labels[predicted_label_index]
print(f"The image is classified as: {predicted_label}")
```

**VisualizaciÃ³n de probabilidades:**

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.bar(labels, probs[0])
plt.title("Image Classification Probabilities")
plt.ylabel("Probability")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

---

## ğŸ“Š Resultados Visuales

![img](clip.gif)

## ğŸ§© Prompts Usados

```text
Â¿CÃ³mo uso CLIP de OpenAI para clasificar imÃ¡genes con descripciones libres en Google Colab?
Â¿CÃ³mo obtengo las probabilidades de correspondencia entre una imagen y varias frases usando CLIP?
Â¿CÃ³mo visualizo los resultados de CLIP en un grÃ¡fico de barras?
```

---

## ğŸ’¬ ReflexiÃ³n Final

- CLIP es una herramienta muy flexible y poderosa para clasificaciÃ³n visual guiada por texto.
- Es posible usar frases naturales, no solo palabras clave, para describir clases y mejorar la interpretaciÃ³n de resultados.
- Como mejora, se podrÃ­an explorar imÃ¡genes propias, comparar distintos modelos de CLIP, o usar etiquetas en espaÃ±ol.

---

## âœ… Checklist de Entrega

- [x] Notebook funcional y comentado
- [x] Ejemplo de imagen y etiquetas personalizadas
- [x] VisualizaciÃ³n de resultados y probabilidades
- [x] README claro y detallado

---