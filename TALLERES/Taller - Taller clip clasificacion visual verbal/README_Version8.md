# ğŸ–¼ï¸ Taller: ClasificaciÃ³n Visual y Verbal con CLIP (OpenAI)

## ğŸ“… Fecha
`2025-06-20`

---

## ğŸ¯ Objetivo del Notebook

Experimentar con el modelo CLIP de OpenAI para realizar clasificaciÃ³n de imÃ¡genes utilizando descripciones textuales en lenguaje natural. El taller permite cargar una imagen y una lista de etiquetas/frases, y obtener la etiqueta mÃ¡s probable segÃºn la comprensiÃ³n conjunta de visiÃ³n y lenguaje de CLIP.

---

## ğŸ§  Conceptos Aprendidos

- [x] Â¿QuÃ© es CLIP y cÃ³mo une visiÃ³n y lenguaje?
- [x] InstalaciÃ³n y uso del modelo pre-entrenado CLIP en Python
- [x] Preprocesamiento de imÃ¡genes y textos para CLIP
- [x] Inferencia: obtenciÃ³n de similitud imagen-texto
- [x] VisualizaciÃ³n de resultados y probabilidades

---

## ğŸ”§ Herramientas y Entornos

- Google Colab (Python 3, GPU opcional)
- [CLIP (OpenAI)](https://github.com/openai/CLIP)
- torch, torchvision, PIL, matplotlib, numpy

---

## ğŸ“ Estructura del Proyecto

```
Taller - Taller clip clasificacion visual verbal/
â””â”€â”€ Taller_clip_clasificacion_visual_verbal.ipynb   # Notebook principal
```

---

## ğŸ§ª ImplementaciÃ³n

### ğŸ”¹ Etapas realizadas

1. InstalaciÃ³n de CLIP y dependencias.
2. Carga del modelo y selecciÃ³n de dispositivo (CPU/GPU).
3. Preprocesamiento de la imagen y las etiquetas/frases.
4. Inferencia: obtenciÃ³n de vectores de imagen y texto, cÃ¡lculo de probabilidades.
5. VisualizaciÃ³n: etiqueta ganadora y grÃ¡fica de probabilidades para todas las frases.

---

### ğŸ”¹ CÃ³digo real usado para clasificaciÃ³n

```python
import clip
import torch
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)
labels = ["a blurry cat", "a fast sports car", "dog", "horse", "a car", "a tree"]
image = preprocess(Image.open("/content/car.jpeg")).unsqueeze(0).to(device)
text = clip.tokenize(labels).to(device)
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    logits_per_image, _ = model(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()
```

**PredicciÃ³n:**

```python
predicted_label_index = np.argmax(probs)
predicted_label = labels[predicted_label_index]
print(f"The image is classified as: {predicted_label}")
```

**VisualizaciÃ³n de probabilidades:**

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.bar(labels, probs[0])
plt.title("Image Classification Probabilities")
plt.ylabel("Probability")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
```

---

## ğŸ“Š Resultados Visuales

![img](img.gif)
---

## ğŸ§© Prompts Usados

```text
Â¿CÃ³mo uso CLIP de OpenAI para clasificar imÃ¡genes con frases personalizadas?
Â¿CÃ³mo obtengo y visualizo las probabilidades de cada etiqueta con CLIP?
Â¿CÃ³mo puedo cargar mis propias imÃ¡genes y frases en CLIP en Colab?
```

---

## ğŸ’¬ ReflexiÃ³n Final

- CLIP permite clasificar imÃ¡genes usando libremente descripciones en lenguaje natural, rompiendo la barrera de las etiquetas fijas.
- Es fÃ¡cil de usar y visualizar, y muy Ãºtil para tareas de prototipado rÃ¡pido en visiÃ³n y lenguaje.
- Como mejora, podrÃ­a usarse con frases en espaÃ±ol, experimentar con imÃ¡genes propias o usar CLIP para tareas de bÃºsqueda cruzada imagen-texto.

---

## âœ… Checklist de Entrega

- [x] Notebook funcional y comentado
- [x] Ejemplo con imagen y frases personalizadas
- [x] VisualizaciÃ³n de resultados y probabilidades
- [x] README claro y detallado

---