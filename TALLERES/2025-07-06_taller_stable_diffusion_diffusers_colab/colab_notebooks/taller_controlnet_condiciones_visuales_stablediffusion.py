# -*- coding: utf-8 -*-
"""taller_controlnet_condiciones_visuales_stablediffusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQtb_fNvk8VzAPSLz8D-9KEF_6FLQgwq
"""

!pip install diffusers transformers accelerate safetensors controlnet_aux --quiet

# no se us√≥
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch

# Cargar ControlNet para Canny
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny",
    torch_dtype=torch.float16
)

# Pipeline con modelo SD + ControlNet
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16
).to("cuda")

from PIL import Image
image = Image.open("/content/ciudadCyberpunkBase.jpeg").convert("RGB")

from controlnet_aux import CannyDetector

canny = CannyDetector()
condition_canny = canny(image)

from controlnet_aux import MidasDetector

# Carga el detector midas con pesos preentrenados
depth = MidasDetector.from_pretrained("lllyasviel/Annotators")
condition_depth = depth(image)

from controlnet_aux import OpenposeDetector

# Carga el detector OpenPose desde weights preentrenados
pose_detector = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
condition_pose = pose_detector(image, hand_and_face=True)

from diffusers import StableDiffusionPipeline, StableDiffusionControlNetPipeline, ControlNetModel
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

# Pipeline solo texto
pipe_txt = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)

# Pipeline con ControlNet (usa el mismo model de antes o rec√°rgalo)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny",
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
)

pipe_ctrl = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)

# Generaci√≥n solo texto
result_text = pipe_txt(
    prompt="A cyberpunk city skyline at night with a car and person with a arm, with neon colors",
    num_inference_steps=30,
    guidance_scale=7.5
).images[0]
result_text.save("only_text.png")

# Generaci√≥n condicionada con ControlNet (ejemplo con canny)
from controlnet_aux import CannyDetector
canny = CannyDetector()
condition_canny = canny(image)  # o una imagen base
result_canny = pipe_ctrl(
    prompt="A cyberpunk city skyline at night with a car and person with a arm, with neon colors",
    image=condition_canny,
    num_inference_steps=30,
    guidance_scale=7.5
).images[0]
result_canny.save("with_canny.png")

import matplotlib.pyplot as plt
from PIL import Image

# üñºÔ∏è Carga de las im√°genes
img_control = Image.open("/content/ciudadCyberpunkBase.jpeg").convert("RGB")
img_text = Image.open("only_text.png")
img_ctrl = Image.open("with_canny.png")

# Configurar figura: 1 fila, 3 columnas
fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(18, 6))

# Mostrar la imagen de condici√≥n
ax0.imshow(img_control)
ax0.set_title("Original para condicionar")
ax0.axis("off")

# Mostrar imagen generada solo por texto
ax1.imshow(img_text)
ax1.set_title("Generada con solo texto")
ax1.axis("off")

# Mostrar imagen generada con ControlNet (Canny)
ax2.imshow(img_ctrl)
ax2.set_title("Generada con ControlNet (Canny)")
ax2.axis("off")

plt.tight_layout()
plt.show()

from diffusers import StableDiffusionPipeline, StableDiffusionControlNetPipeline, ControlNetModel
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

# Pipeline solo texto
pipe_txt = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)

# Pipeline con ControlNet (usa el mismo model de antes o rec√°rgalo)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny",
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
)

pipe_ctrl = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)

# Generaci√≥n solo texto
result_text = pipe_txt(
    prompt="the downtown from cyberpunk city at night with a car and person with a arm, with neon colors and with fish holograms at the center",
    num_inference_steps=30,
    guidance_scale=7.5
).images[0]
result_text.save("only_text2.png")

# Generaci√≥n condicionada con ControlNet (ejemplo con canny)
from controlnet_aux import CannyDetector
canny = CannyDetector()
condition_canny = canny(image)  # o una imagen base
result_canny = pipe_ctrl(
    prompt="the downtown from cyberpunk city at night with a car and person with a arm, with neon colors and with fish holograms at the center",
    image=condition_canny,
    num_inference_steps=30,
    guidance_scale=7.5
).images[0]
result_canny.save("with_canny2.png")

import matplotlib.pyplot as plt
from PIL import Image

# üñºÔ∏è Carga de las im√°genes
img_control = Image.open("/content/ciudadCyberpunkBase.jpeg").convert("RGB")
img_text = Image.open("only_text2.png")
img_ctrl = Image.open("with_canny2.png")

# Configurar figura: 1 fila, 3 columnas
fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(18, 6))

# Mostrar la imagen de condici√≥n
ax0.imshow(img_control)
ax0.set_title("Original para condicionar")
ax0.axis("off")

# Mostrar imagen generada solo por texto
ax1.imshow(img_text)
ax1.set_title("Generada con solo texto")
ax1.axis("off")

# Mostrar imagen generada con ControlNet (Canny)
ax2.imshow(img_ctrl)
ax2.set_title("Generada con ControlNet (Canny)")
ax2.axis("off")

plt.tight_layout()
plt.show()

from controlnet_aux import CannyDetector, MidasDetector, OpenposeDetector

canny = CannyDetector()
depth = MidasDetector.from_pretrained("lllyasviel/Annotators")
pose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")

cond_canny = canny(image)
cond_depth = depth(image)
cond_pose = pose(image, hand_and_face=True)

for cond, label in [(cond_canny, "Canny"), (cond_depth, "Depth"), (cond_pose, "Pose")]:
    img = pipe_ctrl(
        prompt="the downtown from cyberpunk city at night with a car and person with a arm, with neon colors and with fish holograms at the center",
        image=cond,
        num_inference_steps=30,
        guidance_scale=7.5
    ).images[0]
    img.save(f"result_{label}.png")

import matplotlib.pyplot as plt
from PIL import Image

# üñºÔ∏è Carga de las im√°genes
img_control = Image.open("result_Pose.png").convert("RGB")
img_text = Image.open("result_Canny.png")
img_ctrl = Image.open("result_Depth.png")

# Configurar figura: 1 fila, 3 columnas
fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(18, 6))

# Mostrar la imagen de condici√≥n
ax0.imshow(img_control)
ax0.set_title("Generada con condici√≥n pose")
ax0.axis("off")

# Mostrar imagen generada solo por texto
ax1.imshow(img_text)
ax1.set_title("Generada con condici√≥n canny")
ax1.axis("off")

# Mostrar imagen generada con ControlNet (Canny)
ax2.imshow(img_ctrl)
ax2.set_title("Generada con condici√≥n (depth)")
ax2.axis("off")

plt.tight_layout()
plt.show()